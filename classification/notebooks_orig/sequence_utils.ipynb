{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python modules\n",
    "import numpy as np\n",
    "\n",
    "# my modules\n",
    "import activation_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method takes a list of numbers. The format of the list is as follows:\n",
    "[n_x, n_h1, n_h2, ..., n_o]\n",
    "where \n",
    "n_x = number of features in the input\n",
    "n_h1, n_h2, ... = number of nodes in first, second, etc. hidden layers\n",
    "n_o = number of output nodes.\n",
    "\n",
    "For example:\n",
    "If layers = [4, 6, 3, 1], then it has:\n",
    "number of features in the input = 4\n",
    "number of nodes in the first hidden layer = 6\n",
    "number of nodes in the second hidden layer = 3\n",
    "number of nodes in the output layer = 1\n",
    "\"\"\"\n",
    "def initialize_parameters(layers):\n",
    "    L = len(layers)\n",
    "    W = []\n",
    "    b = []\n",
    "\n",
    "    for l in range(0, L - 1):\n",
    "        w_l = np.random.randn(layers[l + 1], layers[l]) * 0.01\n",
    "        b_l = np.zeros((layers[l + 1], 1))\n",
    "        W.append(w_l)\n",
    "        b.append(b_l) \n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, W, b):\n",
    "    \"\"\"\n",
    "    We use a cache to store necessary intermediate metrics for each layers.\n",
    "    In this cache, for each layer, we store the corresponding input (AL - 1), weight(WL),\n",
    "    bias(bL) and linear_forward(zL).\n",
    "    \n",
    "    We store this intermediate metrics, because we will need these for evaulating derivates\n",
    "    during back-propagation.\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    L = len(W)\n",
    "    for l in range(L):\n",
    "        if l == 0:\n",
    "            input = X\n",
    "        else:\n",
    "            input = a\n",
    "        z = np.dot(W[l], input) + b[l]\n",
    "        if l < L - 1:\n",
    "            a = relu(z)\n",
    "        else:\n",
    "            a = sigmoid(z)\n",
    "        \"\"\"\n",
    "        Here we are creating a cache for the current layer\n",
    "        and appending to the caches.\n",
    "        \"\"\"\n",
    "        cache = (input, W[l], b[l], z)\n",
    "        caches.append(cache)\n",
    "    return a, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = AL.shape[1]\n",
    "    # will have to check for why the commented cost function was returning (10, 10)\n",
    "#     return (-1 / m) * (np.dot(Y, np.log(AL.T)) + (np.dot(1 - Y, np.log(1 - AL.T))))\n",
    "    cost = -(1 / m) * np.sum((Y * np.log(AL) + (1 - Y) * np.log(1 - AL)))\n",
    "    cost=np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_back(A_prev, W, b, dZ):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "During back propagation, we are going to calculate the derivatives (dA_prev, dW, db and dZ),\n",
    "which will help us to improve our parameters (W and b).\n",
    "\"\"\"\n",
    "def back_prop(AL, Y, caches):\n",
    "    L = len(caches)  # number of layers\n",
    "    m = AL.shape[1]\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    grads = []\n",
    "\n",
    "    for l in reversed(range(L)):\n",
    "        A_prev, W, b, Z = caches[l]\n",
    "        if l == L-1:\n",
    "            dZ = sigmoid_back(dAL, Z)\n",
    "        else:\n",
    "            dZ = relu_back(dA_prev, Z)\n",
    "\n",
    "        dA_prev, dW, db = linear_back(A_prev, W, b, dZ)\n",
    "        grad = (dA_prev, dW, db)\n",
    "        \"\"\"\n",
    "        Because we are moving backward, we would need to store the gradient descent (GD)\n",
    "        for each layers properly. That is, even if we moving from L->L-1->...->1,\n",
    "        the grads (list containing GDs) in 1->...->L-1->L order.\n",
    "        (This approach will help us, while updating the weight(W) and bias(b) parameters\n",
    "        for next iteration.)\n",
    "        \n",
    "        Therefore, instead of append, I have used insert(0, grad) below.\n",
    "        \"\"\"\n",
    "        #grads.append(grad)\n",
    "        grads.insert(0, grad)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W, b, grads, learning_rate):\n",
    "    L = len(grads)\n",
    "    for l in range(L):\n",
    "        dA_prev, dW, db = grads[l]\n",
    "        W[l] = W[l] - learning_rate * dW\n",
    "        b[l] = b[l] - learning_rate * db\n",
    "    return W, b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
