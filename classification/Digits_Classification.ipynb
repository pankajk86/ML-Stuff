{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      "Before Transpose: X: (1617, 64), y: (1617, 1)\n",
      "After Transpose: X: (64, 1617), y: (1, 1617)\n",
      "X_train: (64, 1617), Y_train: (10, 1617)\n",
      "X_test: (64, 180), Y_test: (10, 180)\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "print(np.unique(digits.target))\n",
    "data = digits.data\n",
    "print(np.unique(data))\n",
    "target = digits.target.reshape(data.shape[0], 1)\n",
    "\n",
    "# scale the feature value(digits.data) between 0 to 1 instead of 0 to 16\n",
    "data = data / 16\n",
    "\n",
    "# create training & test data set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, \n",
    "                                                    test_size = 0.1, random_state = 0)\n",
    "\n",
    "\"\"\"\n",
    "Dimension of input(X) and y(output) should be as follows:\n",
    "X:= (features, samples). i.e. if input has 1000 samples with 64 features in each sample,\n",
    "X should be arranged in (64, 1000) matrix. If input is provided in (1000, 64), \n",
    "then transpose it.\n",
    "\n",
    "Y:= (1, samples). Similar to X, each output should be stacked column-wise.\n",
    "\"\"\"\n",
    "print(f'Before Transpose: X: {X_train.shape}, y: {Y_train.shape}')\n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.T\n",
    "X_test = X_test.T\n",
    "Y_test = Y_test.T\n",
    "\n",
    "print(f'After Transpose: X: {X_train.shape}, y: {Y_train.shape}')\n",
    "\n",
    "\"\"\"\n",
    "Currently, Y_train and Y_test are of (1, train_sample_size) and (1, test_sample_size).\n",
    "However, we know that it takes a value from 0 to 9. Therefore, we would need to convert\n",
    "these labels (Y's) to (10, train_sample_size) and (10, test_sample_size).\n",
    "\n",
    "Here, each row will for the matrix will contain one 1 and rest 0, with 1 denoting the index\n",
    "for the actual label value.\n",
    "\"\"\"\n",
    "m_train = Y_train.shape[1]\n",
    "Y_train_temp = np.zeros((10, m_train))\n",
    "for i in range(m_train):\n",
    "    Y_train_temp[Y_train[0, i], i] = 1\n",
    "Y_train = Y_train_temp\n",
    "    \n",
    "m_test = Y_test.shape[1]\n",
    "Y_test_temp = np.zeros((10, m_test))\n",
    "for i in range(m_test):\n",
    "    Y_test_temp[Y_test[0, i], i] = 1\n",
    "Y_test = Y_test_temp\n",
    "\n",
    "print(f'X_train: {X_train.shape}, Y_train: {Y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, Y_test: {Y_test.shape}')\n",
    "    \n",
    "# print(Y[:, 0:5])\n",
    "# print(Y_train[:, 0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([4, 1, 0, 0, 4, 3, 5])\n",
    "aa = np.zeros((10, len(a)))\n",
    "for i in range(len(a)):\n",
    "    aa[a[i], i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method takes parameter values from 0 to 1796\n",
    "\"\"\"\n",
    "def print_details(sample_id):\n",
    "    x = X_train[:, sample_id].reshape(8, 8)\n",
    "    y = Y_train[:, sample_id]\n",
    "    print(f'X: {x},\\n y: {y}')\n",
    "    plt.gray()\n",
    "    plt.matshow(x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[0.     0.     0.     0.5625 0.9375 0.125  0.     0.    ]\n",
      " [0.     0.     0.3125 1.     0.6875 0.0625 0.     0.    ]\n",
      " [0.     0.     0.8125 0.9375 0.0625 0.     0.     0.    ]\n",
      " [0.     0.125  1.     0.6875 0.     0.     0.     0.    ]\n",
      " [0.     0.125  1.     0.6875 0.25   0.25   0.     0.    ]\n",
      " [0.     0.125  0.9375 1.     1.     0.875  0.625  0.0625]\n",
      " [0.     0.     0.5625 1.     0.4375 0.1875 0.9375 0.375 ]\n",
      " [0.     0.     0.     0.4375 0.9375 1.     1.     0.375 ]],\n",
      " y: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print details of any sample out of the available samples\n",
    "print_details(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The following activation functions are used in forward propagation.\"\"\"\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    return np.exp(Z)/sum(np.exp(Z))\n",
    "\n",
    "\"\"\"The following derivatives for activation functions are used back propagation.\"\"\"\n",
    "def sigmoid_back(dA, Z):\n",
    "    S = sigmoid(Z)\n",
    "    dS = S * (1 - S)\n",
    "    dZ = dA * dS     # chain rule\n",
    "    return dZ\n",
    "\n",
    "def relu_back(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def softmax_back(dA, Z):\n",
    "    # need to implement\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method takes a list of numbers. The format of the list is as follows:\n",
    "[n_x, n_h1, n_h2, ..., n_o]\n",
    "where \n",
    "n_x = number of features in the input\n",
    "n_h1, n_h2, ... = number of nodes in first, second, etc. hidden layers\n",
    "n_o = number of output nodes.\n",
    "\n",
    "For example:\n",
    "If layers = [4, 6, 3, 1], then it has:\n",
    "number of features in the input = 4\n",
    "number of nodes in the first hidden layer = 6\n",
    "number of nodes in the second hidden layer = 3\n",
    "number of nodes in the output layer = 1\n",
    "\"\"\"\n",
    "def initialize_parameters(layers):\n",
    "    L = len(layers)\n",
    "    W = []\n",
    "    b = []\n",
    "\n",
    "    for l in range(0, L - 1):\n",
    "        w_l = np.random.randn(layers[l + 1], layers[l]) * 0.01\n",
    "        b_l = np.zeros((layers[l + 1], 1))\n",
    "        W.append(w_l)\n",
    "        b.append(b_l) \n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: W: (12, 64), b: (12, 1)\n",
      "layer 1: W: (8, 12), b: (8, 1)\n",
      "layer 2: W: (10, 8), b: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "layers = [64, 12, 8, 10]\n",
    "W, b = initialize_parameters(layers)\n",
    "\n",
    "for l in range(len(layers) - 1):\n",
    "    print(f'layer {l}: W: {W[l].shape}, b: {b[l].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, W, b):\n",
    "    \"\"\"\n",
    "    We use a cache to store necessary intermediate metrics for each layers.\n",
    "    In this cache, for each layer, we store the corresponding input (AL - 1), weight(WL),\n",
    "    bias(bL) and linear_forward(zL).\n",
    "    \n",
    "    We store this intermediate metrics, because we will need these for evaulating derivates\n",
    "    during back-propagation.\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    L = len(W)\n",
    "    for l in range(L):\n",
    "        if l == 0:\n",
    "            input = X\n",
    "        else:\n",
    "            input = a\n",
    "        z = np.dot(W[l], input) + b[l]\n",
    "        if l < L - 1:\n",
    "            a = relu(z)\n",
    "        else:\n",
    "            a = sigmoid(z)\n",
    "        \"\"\"\n",
    "        Here we are creating a cache for the current layer\n",
    "        and appending to the caches.\n",
    "        \"\"\"\n",
    "        cache = (input, W[l], b[l], z)\n",
    "        caches.append(cache)\n",
    "    return a, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: (10, 1617), cache length: 3\n",
      "==================\n",
      "layer 0: input: (64, 1617), w: (12, 64), b: (12, 1), z: (12, 1617)\n",
      "layer 1: input: (12, 1617), w: (8, 12), b: (8, 1), z: (8, 1617)\n",
      "layer 2: input: (8, 1617), w: (10, 8), b: (10, 1), z: (10, 1617)\n"
     ]
    }
   ],
   "source": [
    "AL, caches = forward_prop(X_train, W, b)\n",
    "print(f'Output: {AL.shape}, cache length: {len(caches)}')\n",
    "print('==================')\n",
    "for layer in range(3):\n",
    "    il, wl, bl, zl = caches[layer]\n",
    "    print(f'layer {layer}: input: {il.shape}, w: {wl.shape}, b: {bl.shape}, z: {zl.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = AL.shape[1]\n",
    "    # will have to check for why the commented cost function was returning (10, 10)\n",
    "#     return (-1 / m) * (np.dot(Y, np.log(AL.T)) + (np.dot(1 - Y, np.log(1 - AL.T))))\n",
    "    cost = -(1 / m) * np.sum((Y * np.log(AL) + (1 - Y) * np.log(1 - AL)))\n",
    "    cost=np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33715100679120125"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = compute_cost(AL, Y_train)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_back(A_prev, W, b, dZ):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches):\n",
    "    L = len(caches)  # number of layers\n",
    "    m = AL.shape[1]\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    grads = []\n",
    "\n",
    "    for l in reversed(range(L)):\n",
    "        A_prev, W, b, Z = caches[l]\n",
    "        if l == L-1:\n",
    "            dZ = sigmoid_back(dAL, Z)\n",
    "        else:\n",
    "            dZ = relu_back(dA_prev, Z)\n",
    "\n",
    "        dA_prev, dW, db = linear_back(A_prev, W, b, dZ)\n",
    "        grad = (dA_prev, dW, db)\n",
    "        \"\"\"\n",
    "        Because we are moving backward, we would need to store the gradient descent (GD)\n",
    "        for each layers properly. That is, even if we moving from L->L-1->...->1,\n",
    "        the grads (list containing GDs) in 1->...->L-1->L order.\n",
    "        (This approach will help us, while updating the weight(W) and bias(b) parameters\n",
    "        for next iteration.)\n",
    "        \n",
    "        Therefore, instead of append, I have used insert(0, grad) below.\n",
    "        \"\"\"\n",
    "        #grads.append(grad)\n",
    "        grads.insert(0, grad)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: dA_prev: (64, 1617), dW: (12, 64), db: (12, 1)\n",
      "layer 1: dA_prev: (12, 1617), dW: (8, 12), db: (8, 1)\n",
      "layer 2: dA_prev: (8, 1617), dW: (10, 8), db: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "grads = back_prop(AL, Y_train, caches)\n",
    "L = len(grads)\n",
    "for l in range(L):\n",
    "    dA_prev, dW, db = grads[l]\n",
    "    print(f'layer {l}: dA_prev: {dA_prev.shape}, dW: {dW.shape}, db: {db.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W, b, grads, learning_rate):\n",
    "    L = len(grads)\n",
    "    for l in range(L):\n",
    "        dA_prev, dW, db = grads[l]\n",
    "        W[l] = W[l] - learning_rate * dW\n",
    "        b[l] = b[l] - learning_rate * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (12, 64), b: (12, 1)\n",
      "W: (8, 12), b: (8, 1)\n",
      "W: (10, 8), b: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "W, b = update_parameters(W, b, grads, 0.1)\n",
    "for l in range(len(grads)):\n",
    "    print(f'W: {W[l].shape}, b: {b[l].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bringing everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers, learning_rate = 0.0075, num_iterations = 50000):\n",
    "    costs = []\n",
    "    L = len(layers) - 1\n",
    "    W, b = initialize_parameters(layers)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # forward propagation\n",
    "        AL, caches = forward_prop(X, W, b)\n",
    "        # compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "        # backward propagation\n",
    "        grads = back_prop(AL, Y, caches)\n",
    "        # update parameters\n",
    "        W, b = update_parameters(W, b, grads, learning_rate)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'Cost after iteration {i}: {cost}')\n",
    "            costs.append(cost)\n",
    "        \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title('Learning Rate: ' + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6.931488694746727\n",
      "Cost after iteration 1000: 3.479400424130829\n",
      "Cost after iteration 2000: 3.2503121372893076\n",
      "Cost after iteration 3000: 3.2501847495080343\n",
      "Cost after iteration 4000: 3.2500579402410628\n",
      "Cost after iteration 5000: 3.249900204944133\n",
      "Cost after iteration 6000: 3.249676371316968\n",
      "Cost after iteration 7000: 3.249322932575999\n",
      "Cost after iteration 8000: 3.248709438627679\n",
      "Cost after iteration 9000: 3.2475306831208157\n",
      "Cost after iteration 10000: 3.244939613155432\n",
      "Cost after iteration 11000: 3.237865647888034\n",
      "Cost after iteration 12000: 3.208594116924885\n",
      "Cost after iteration 13000: 3.0772756079957593\n",
      "Cost after iteration 14000: 2.8807704031334147\n",
      "Cost after iteration 15000: 2.7873034077570487\n",
      "Cost after iteration 16000: 2.7387288357894817\n",
      "Cost after iteration 17000: 2.7042446713038895\n",
      "Cost after iteration 18000: 2.674529364385176\n",
      "Cost after iteration 19000: 2.6468114299290093\n",
      "Cost after iteration 20000: 2.6144540846868503\n",
      "Cost after iteration 21000: 2.562865619286266\n",
      "Cost after iteration 22000: 2.484618101004531\n",
      "Cost after iteration 23000: 2.3996189377746577\n",
      "Cost after iteration 24000: 2.3103727456669154\n",
      "Cost after iteration 25000: 2.183989023492992\n",
      "Cost after iteration 26000: 2.0552981594672777\n",
      "Cost after iteration 27000: 1.8441308091399224\n",
      "Cost after iteration 28000: 1.665989080195091\n",
      "Cost after iteration 29000: 1.5304093262706724\n",
      "Cost after iteration 30000: 1.4168438763873918\n",
      "Cost after iteration 31000: 1.323599123185984\n",
      "Cost after iteration 32000: 1.2336523545128806\n",
      "Cost after iteration 33000: 1.1256708612234716\n",
      "Cost after iteration 34000: 0.9896601019341822\n",
      "Cost after iteration 35000: 0.8716818684393831\n",
      "Cost after iteration 36000: 0.7751472344773657\n",
      "Cost after iteration 37000: 0.6872489224543076\n",
      "Cost after iteration 38000: 0.6127398783300382\n",
      "Cost after iteration 39000: 0.553188356704874\n",
      "Cost after iteration 40000: 0.504855011131657\n",
      "Cost after iteration 41000: 0.4655533866330862\n",
      "Cost after iteration 42000: 0.4331365094767565\n",
      "Cost after iteration 43000: 0.40632783825629204\n",
      "Cost after iteration 44000: 0.38393719401718995\n",
      "Cost after iteration 45000: 0.3643244072003391\n",
      "Cost after iteration 46000: 0.3466408958761058\n",
      "Cost after iteration 47000: 0.3308472081971337\n",
      "Cost after iteration 48000: 0.3162566989632876\n",
      "Cost after iteration 49000: 0.3030705288127819\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers = [64, 12, 8, 10]\n",
    "W, b = L_layer_model(X_train, Y_train, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_L_layer(X, W, b):\n",
    "    AL, caches = forward_prop(X, W, b)\n",
    "    prediction = np.argmax(AL, axis = 0)\n",
    "    return prediction.reshape(1, prediction.shape[0])\n",
    "\n",
    "def calculate_accuracy(Y_actual, Y_predicted):\n",
    "    print(f'Y: {Y_actual.shape}, Predicted: {Y_predicted.shape}')\n",
    "    Y_temp = np.argmax(Y_actual, axis = 0) # convert (10, m) matrix to (1, m) matrix\n",
    "    Y_actual = Y_temp.reshape(1, Y_temp.shape[0])  # bring it to same dimension as Y_predicted\n",
    "    return np.sum(Y_predicted == Y_actual) / Y_predicted.shape[1] * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training Result-----\n",
      "Y: (10, 1617), Predicted: (1, 1617)\n",
      "97.1552257266543\n",
      "-----Test Result-----\n",
      "Y: (10, 180), Predicted: (1, 180)\n",
      "93.88888888888889\n"
     ]
    }
   ],
   "source": [
    "predict_train = predict_L_layer(X_train, W, b)\n",
    "print('-----Training Result-----')\n",
    "result = calculate_accuracy(Y_train, predict_train)\n",
    "print(result)\n",
    "print('-----Test Result-----')\n",
    "predict_test = predict_L_layer(X_test, W, b)\n",
    "result = calculate_accuracy(Y_test, predict_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
